"""
Created on Mon Aug 30 11:25:28 2021

@author: GC
* revised on 31.08.2021
"""
import pyblp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy as sp
from scipy import stats
from openpyxl import load_workbook

pyblp.options.digits = 4 #this is set at 2 in the tutorial
pyblp.options.verbose = False
pyblp.__version__


###############################################################################
# PT - Base Regressions T3
###############################################################################

#Base - off
###############################################################################
cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
product_data = vfi.loc[(vfi.block == "off"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']
del product_data['rampc']

X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results

elasticities = results.compute_elasticities
elasticities


cd "C:\...\Replication\Python\Base\off_b"

import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("offb.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("offb.txt", "rb") as myFile:
    offb = pickle.load(myFile)

#pvalues
a = offb
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p, this is nrows from dataset minus coefficients (5)
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

offb= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats1.xlsx') as writer:
    offb.to_excel(writer, sheet_name = 'offb', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Base\off_b\stats1.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()
writer.close()


#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Base\off_b\stats1.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)


#Base - p1
###############################################################################
cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
product_data = vfi.loc[(vfi.block == "peak1"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']
del product_data['rampc']

X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results

elasticities = results.compute_elasticities
elasticities

#Post-estimation results

cd "C:...\Replication\Python\Base\p1_b"

import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("p1b.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("p1b.txt", "rb") as myFile:
    p1b = pickle.load(myFile)

#pvalues
a = p1b
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p, this is nrows from dataset minus coefficients (5)
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

p1b= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats2.xlsx') as writer:
    p1b.to_excel(writer, sheet_name = 'p1b', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Base\p1_b\stats2.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()


#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Base\p1_b\stats2.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)


#Base - p2
###############################################################################
cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
product_data = vfi.loc[(vfi.block == "peak2"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']
del product_data['rampc']

X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results

elasticities = results.compute_elasticities
elasticities

#Post-estimation results
cd "C:\...\Replication\Python\Base\p2_b"	
import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("p2b.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("p2b.txt", "rb") as myFile:
    p2b = pickle.load(myFile)

#pvalues
a = p2b
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p, this is nrows from dataset minus coefficients (5)
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

p1b= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats3.xlsx') as writer:
    p1b.to_excel(writer, sheet_name = 'p2b', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Base\p2_b\stats3.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()


#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Base\p2_b\stats3.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)

###############################################################################
# PT - Ramping Regressions T3
###############################################################################

#Ramp - off
###############################################################################
#Maquina2

#Ramp - p1
###############################################################################
cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
product_data = vfi.loc[(vfi.block == "peak1"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']


X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa + rampc') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results


elasticities = results.compute_elasticities
elasticities

#Post-estimation results

cd "C:\...\Replication\Python\Ramp\p1_r"
import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("p1r.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("p1r.txt", "rb") as myFile:
    p1r = pickle.load(myFile)

#pvalues
a = p1r
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'rampc', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

p1r= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats5.xlsx') as writer:
    p1r.to_excel(writer, sheet_name = 'p1r', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa', 'rampc'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Ramp\p1_r\stats5.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()


#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Ramp\p1_r\stats5.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)


#Ramp - p2
###############################################################################
cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
product_data = vfi.loc[(vfi.block == "peak2"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']


X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa + rampc') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results


elasticities = results.compute_elasticities
elasticities

#Post-estimation results

cd "C:\...\Replication\Python\Ramp\p2_r"
import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("p2r.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("p2r.txt", "rb") as myFile:
    p2r = pickle.load(myFile)

#pvalues
a = p2r
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'rampc', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

p2r= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats6.xlsx') as writer:
    p2r.to_excel(writer, sheet_name = 'p2r', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa', 'rampc'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Ramp\p2_r\stats6.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()
writer.close()

#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Ramp\p2_r\stats6.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)



###############################################################################
# PT - Counterfactual 1 Table X - inc. ramping
###############################################################################

#CF1- off 0-10
###############################################################################

cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
vfi1 = vfi[(vfi['supply_instruments2'] <10)]
product_data = vfi1.loc[(vfi1.block == "off"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']


X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa + rampc') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results


elasticities = results.compute_elasticities
elasticities

#Post-estimation results

cd "C:\...\Replication\Python\Ramp\CF1\0-10\off_cf11"
import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("off_cf11.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("off_cf11.txt", "rb") as myFile:
   off_cf11  = pickle.load(myFile)

#pvalues
a = off_cf11
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'rampc', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

off_cf11= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats7.xlsx') as writer:
    off_cf11.to_excel(writer, sheet_name = 'off_cf11', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa', 'rampc'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Ramp\CF1\0-10\off_cf11\stats7.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()


#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Ramp\CF1\0-10\off_cf11\stats7.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)


#CF1- p1 0-10
###############################################################################

cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
vfi1 = vfi[(vfi['supply_instruments2'] <10)]
product_data = vfi1.loc[(vfi1.block == "peak1"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']


X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa + rampc') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results


elasticities = results.compute_elasticities
elasticities

#Post-estimation results

cd "C:\...\Replication\Python\Ramp\CF1\0-10\p1_cf11"
import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("p1_cf11.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("p1_cf11.txt", "rb") as myFile:
   p1_cf11  = pickle.load(myFile)

#pvalues
a = p1_cf11
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'rampc', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

p1_cf11= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats8.xlsx') as writer:
    p1_cf11.to_excel(writer, sheet_name = 'p1_cf11', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa', 'rampc'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Ramp\CF1\0-10\p1_cf11\stats8.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()
writer.close()


#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Ramp\CF1\0-10\p1_cf11\stats8.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)


#CF1- p2 0-10
###############################################################################

cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
vfi1 = vfi[(vfi['supply_instruments2'] <10)]
product_data = vfi1.loc[(vfi1.block == "peak2"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']


X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa + rampc') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results


elasticities = results.compute_elasticities
elasticities

#Post-estimation results

cd "C:\...\Replication\Python\Ramp\CF1\0-10\p2_cf11"
import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("p2_cf11.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("p2_cf11.txt", "rb") as myFile:
   p2_cf11  = pickle.load(myFile)

#pvalues
a = p2_cf11
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'rampc', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

p2_cf11= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats9.xlsx') as writer:
    p2_cf11.to_excel(writer, sheet_name = 'p2_cf11', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa', 'rampc'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Ramp\CF1\0-10\p2_cf11\stats9.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()


#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Ramp\CF1\0-10\p2_cf11\stats9.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)



#CF1- off 10-20
###############################################################################

cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
vfi2 = vfi[(vfi['supply_instruments2'] >=10) & (vfi['supply_instruments2'] <20)]
product_data = vfi2.loc[(vfi2.block == "off"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']


X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa + rampc') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results


elasticities = results.compute_elasticities
elasticities

#Post-estimation results

cd "C:\...\Replication\Python\Ramp\CF1\10-20\off_cf12"
import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("off_cf12.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("off_cf12.txt", "rb") as myFile:
   off_cf12  = pickle.load(myFile)

#pvalues
a = off_cf12
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'rampc', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

off_cf11= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats10.xlsx') as writer:
    off_cf12.to_excel(writer, sheet_name = 'off_cf12', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa', 'rampc'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Ramp\CF1\10-20\off_cf12\stats10.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()


#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Ramp\CF1\10-20\off_cf12\stats10.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)


#CF1- p1 10-20
###############################################################################

cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
vfi2 = vfi[(vfi['supply_instruments2'] >=10) & (vfi['supply_instruments2'] <20)]
product_data = vfi2.loc[(vfi.block == "peak1"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']


X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa + rampc') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results


elasticities = results.compute_elasticities
elasticities

#Post-estimation results

cd "C:\...\Replication\Python\Ramp\CF1\10-20\p1_cf12"
import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("p1_cf12.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("p1_cf12.txt", "rb") as myFile:
   p1_cf12  = pickle.load(myFile)

#pvalues
a = p1_cf12
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'rampc', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

p1_cf12= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats11.xlsx') as writer:
    p1_cf12.to_excel(writer, sheet_name = 'p1_cf12', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa', 'rampc'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Ramp\CF1\10-20\p1_cf12\stats11.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()
writer.close()

#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Ramp\CF1\10-20\p1_cf12\stats11.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)


#CF1- p2 10-20
###############################################################################

cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
vfi2 = vfi[(vfi['supply_instruments2'] >=10) & (vfi['supply_instruments2'] <20)]
product_data = vfi2.loc[(vfi2.block == "peak2"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']


X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa + rampc') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results


elasticities = results.compute_elasticities
elasticities

#Post-estimation results

cd "C:\...\Replication\Python\Ramp\CF1\10-20\p2_cf12"
import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("p2_cf12.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("p2_cf12.txt", "rb") as myFile:
   p2_cf12  = pickle.load(myFile)

#pvalues
a = p2_cf12
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'rampc', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

p2_cf11= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats12.xlsx') as writer:
    p2_cf12.to_excel(writer, sheet_name = 'p2_cf12', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa', 'rampc'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Ramp\CF1\10-20\p2_cf12\stats12.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()


#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Ramp\CF1\10-20\p2_cf12\stats12.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)


#CF1- off 20-
###############################################################################

cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
vfi3 = vfi[(vfi['supply_instruments2'] <=20)]
product_data = vfi3.loc[(vfi3.block == "off"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']


X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa + rampc') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results


elasticities = results.compute_elasticities
elasticities

#Post-estimation results

cd "C:\...\Replication\Python\Ramp\CF1\20-\off_cf13"
import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("off_cf13.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("off_cf13.txt", "rb") as myFile:
   off_cf13  = pickle.load(myFile)

#pvalues
a = off_cf13
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'rampc', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

off_cf13= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats13.xlsx') as writer:
    off_cf13.to_excel(writer, sheet_name = 'off_cf13', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa', 'rampc'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Ramp\CF1\20-\off_cf13\stats13.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()
writer.close()


#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Ramp\CF1\20-\off_cf13\stats13.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)


#CF1- p1 20-
###############################################################################

cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
vfi3 = vfi[(vfi['supply_instruments2'] <=20)]
product_data = vfi3.loc[(vfi3.block == "peak1"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']


X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa + rampc') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results


elasticities = results.compute_elasticities
elasticities

#Post-estimation results

cd "C:\...\Replication\Python\Ramp\CF1\20-\p1_cf13"
import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("p1_cf13.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("p1_cf13.txt", "rb") as myFile:
   p1_cf13  = pickle.load(myFile)

#pvalues
a = off_cf13
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'rampc', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

off_cf11= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats14.xlsx') as writer:
    p1_cf13.to_excel(writer, sheet_name = 'p1_cf13', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa', 'rampc'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() #esto estaba vacio
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Ramp\CF1\20-\p1_cf13\stats14.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()


#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Ramp\CF1\20-\p1_cf13\stats14.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)


#CF1- p2 20-
###############################################################################

cd "C:\...\Replication\R_paneldata\BLP"	

vfi = pd.read_csv(r'i1rv.cvs', index_col=False)
vfi3 = vfi[(vfi['supply_instruments2'] <=20)]
product_data = vfi3.loc[(vfi3.block == "peak2"), ]
product_data = product_data.drop(columns=["nesting_ids"])

del product_data['Unnamed: 0']


X1 = pyblp.Formulation('0 + prices') 
X2 = pyblp.Formulation('0  + loadf') 
X3 = pyblp.Formulation('0 + fcost + fcoa + rampc') 

logit_formulations = (
    X1,
    X2,
    X3
   )
#logit_formulations

mc_integration = pyblp.Integration('monte_carlo', size=50, specification_options={'seed' : 0})
#mc_integration

problem = pyblp.Problem(logit_formulations, product_data, integration = mc_integration)
#problem

bfgs = pyblp.Optimization('bfgs', {'gtol' : 1e-10})
#bfgs

results = problem.solve(sigma=np.ones((1,1)),
                        beta = (0.5),
                        optimization = bfgs,
                        costs_bounds = (24.999, None))
results


elasticities = results.compute_elasticities
elasticities

#Post-estimation results

cd "C:\...\Replication\Python\Ramp\CF1\20-\p2_cf13"
import pickle
a = pyblp.ProblemResults.to_dict(results)
a.keys()
with open("p2_cf13.txt", "wb") as myFile:
    pickle.dump(a, myFile)

import pickle
with open("p2_cf13.txt", "rb") as myFile:
   p2_cf13  = pickle.load(myFile)

#pvalues
a = p2_cf13
se = list([a[key] for key in ['gamma_se', 'beta_se', 'sigma_se']])
se = np.concatenate(se, axis=0 )

coeff = list([a[key] for key in ['gamma', 'beta', 'sigma']])
coeff = np.concatenate(coeff, axis=0 )
v = list([a[key] for key in ['xi']])
v = np.concatenate(v, axis=0 )
n=len(v)-5 # degrees of freedom n-p
ts= coeff/se

pval = (1-stats.t.cdf(np.abs(ts),n))*2 
interval = stats.norm.interval(0.68, loc=coeff, scale=se/np.sqrt(n))
CImin = interval[0]
CImax = interval[1]


#panda stats
index = ['fcost', 'co2', 'rampc', 'price', 'loadf']

df1 = pd.DataFrame(coeff, index=index, columns=['coeff'])
df2 = pd.DataFrame(se, index=index, columns=['se'])
df3 = pd.DataFrame(pval, index=index, columns=['p-val'])
df4 = pd.DataFrame(CImin, index= index, columns=['CImin'])
df5 = pd.DataFrame(CImax, index= index, columns=['CImax'])

p2_cf13= pd.concat([df1, df2, df3, df4, df5], axis=1)

#additional info
obj = list([a[key] for key in ['objective']])
obj = pd.DataFrame(np.concatenate(obj, axis=0 ), columns=['obj'])

markets = list([a[key] for key in ['contraction_evaluations']])
markets = np.concatenate(markets, axis=0 )
T = pd.Series(len(markets))

add= pd.concat([obj,T], axis=1)

with pd.ExcelWriter('stats15.xlsx') as writer:
    p2_cf13.to_excel(writer, sheet_name = 'p2_cf13', index = True)
    add.to_excel(writer, sheet_name = 'add', index = True)


#build an excel file that contains T market, prices, costs, shares, CS and PS
n2 = len(product_data)
stamp = pd.DataFrame(product_data, columns=['market_ids', 'block', 'hour', 'year', 'plant', 'prices', 'shares', 'fcost', 'fcoa', 'rampc'])
stamp.index = range(n2)
costs = results.compute_costs()
markups = results.compute_markups(costs=costs) #or here it is faster to use the previous calculation
profits = results.compute_profits(costs=costs)
cs = results.compute_consumer_surpluses() 
c= pd.DataFrame(costs, columns=['costs'])
cc = list([a[key] for key in ['clipped_costs']])
cc = pd.DataFrame(np.concatenate(cc, axis=0 ),columns=['clippedc'] )
d= pd.DataFrame(markups, columns=['mks'])
profitss=pd.DataFrame(profits, columns=['ps'])
consumers=pd.DataFrame(cs, columns=['cs']) 

#merging
stampt = stamp[0:len(markets)]
merge = pd.concat([stamp, c,cc, d , profitss], axis=1)
merge2 = pd.concat([stampt, consumers], axis=1)

path = r"C:\...\Replication\Python\Ramp\CF1\20-\p2_cf13\stats15.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
merge.to_excel(writer, sheet_name = 'one', index = True)
merge2.to_excel(writer, sheet_name = 'two', index = True)
writer.save()
writer.close()

#curvature
elasticities = results.compute_elasticities() 

aggregates = results.compute_aggregate_elasticities(factor=0.1)
aggregates2 = results.compute_aggregate_elasticities(factor=0.2)
aggregates3 = results.compute_aggregate_elasticities(factor=0.3)
aggregates4 = results.compute_aggregate_elasticities(factor=0.4)
aggregates5 = results.compute_aggregate_elasticities(factor=0.5)
aggregates6 = results.compute_aggregate_elasticities(factor=0.6)
aggregates7 = results.compute_aggregate_elasticities(factor=0.7)
aggregates8 = results.compute_aggregate_elasticities(factor=0.8)

agg = pd.DataFrame(aggregates,columns=['agg0.1'] )
agg2 = pd.DataFrame(aggregates2,columns=['agg0.2'] )
agg3 = pd.DataFrame(aggregates3,columns=['agg0.3'] )
agg4 = pd.DataFrame(aggregates4,columns=['agg0.4'] )
agg5 = pd.DataFrame(aggregates5,columns=['agg0.5'] )
agg6 = pd.DataFrame(aggregates6,columns=['agg0.6'] )
agg7 = pd.DataFrame(aggregates7,columns=['agg0.7'] )
agg8 = pd.DataFrame(aggregates8,columns=['agg0.8'] )

curvature = pd.concat([agg,agg2,agg3,agg4,agg5,agg6,agg7,agg8 ], axis=1, sort=False)

path = r"C:\...\Replication\Python\Ramp\CF1\20-\p2_cf13\stats15.xlsx"
book = load_workbook(path)
writer = pd.ExcelWriter(path, engine = 'openpyxl')
writer.book = book
curvature.to_excel(writer, sheet_name = 'curv', index = True)
writer.save()

#additional data
diversions = results.compute_diversion_ratios()
means = results.extract_diagonal_means(elasticities)
e=pd.DataFrame(elasticities)
di=pd.DataFrame(diversions) 
ie1_o = pd.concat([e, di], axis=1, sort=False)
ie1_o.to_csv('i_1.csv', index = False)

##############################
#Pass-through
import os
import pandas as pd

cwd = os.path.abspath('C:\\...\\Replication\\Python') 
files = os.listdir(cwd) 

pt = pd.DataFrame()
for file in files:
     if file.endswith('.xlsx'):
         pt = pt.append(pd.read_excel(file), ignore_index=True) 

pt.to_excel(r'C:\...\Replication\Python\All\pt.xlsx', index = False)

#Consumer surplus
cwd = os.path.abspath('C:\\...\\Replication\\Python') 
files = os.listdir(cwd) 

cs = pd.DataFrame()
for file in files:
     if file.endswith('.xlsx'):
         cs = cs.append(pd.read_excel(file, sheet_name="two"), ignore_index=True) 

cs.to_excel(r'C:\...\Replication\Python\All\cs.xlsx', index = False)


 #Producer surplus
cwd = os.path.abspath('C:\\...\\Replication\\Python') 
files = os.listdir(cwd) 

ps = pd.DataFrame()
for file in files:
     if file.endswith('.xlsx'):
         ps = ps.append(pd.read_excel(file, sheet_name="one"), ignore_index=True) 
         
ps.to_csv(r'C:\...\Replication\Python\All\ps.csv', index = False)


#Curvature
cwd = os.path.abspath('C:\\...\\Replication\\Python\\Curvature') 
files = os.listdir(cwd) 

curva = pd.DataFrame()
for file in files:
     if file.endswith('.xlsx'):
         curva = curva.append(pd.read_excel(file, sheet_name="curv"), ignore_index=True) 

curva.to_excel(r'C:\...\Replication\Python\All\curva.xlsx', index = False)

